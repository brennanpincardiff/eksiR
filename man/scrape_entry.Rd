% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/scrape_entry.R
\name{scrape_entry}
\alias{scrape_entry}
\title{Scrape data by entry_id}
\usage{
scrape_entry(entry_id, user_agent = curl_user_agent,
  export_csv = FALSE, sleep_time = 0.05)
}
\arguments{
\item{entry_id}{Integer. The entry ID that will be scraped.}

\item{user_agent}{Character. Used to set the user header string in the cURL request. As default, a generic one provided as per https://stackoverflow.com/a/31597823; however, users can provide a custom user agent.}

\item{export_csv}{Logical. If **TRUE** exports the tibble to a csv file.}

\item{sleep_time}{Numeric. A value to set Sys.sleep. Useful to not overwhelm the servers when scraping at scale. Defaults to 0.05.}
}
\value{
Always returns a tibble with same columns. If cURL request returns an error (such as if entry is deleted or servers are temporarily unavailable), returns a tibble with missing values filled with NA.
}
\description{
Scrape data by entry_id
}
\examples{
scrape_entry(1)
scrape_entry(1, export_csv=TRUE)
}
